{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "experienced-buddy",
   "metadata": {},
   "source": [
    "### Instructions to fill the Wikibase instance start [here](#Use-dictionaries-to-populate-your-Wikibase)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrong-preview",
   "metadata": {},
   "source": [
    "# To Install the WikiBase follow the RaiseWikibase install instructions: https://github.com/UB-Mannheim/RaiseWikibase\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crazy-chain",
   "metadata": {},
   "source": [
    "### Requirements:\n",
    "\n",
    "0. Python 3 (.9 or later, lest you need to order dictionaries on your own); pip3\n",
    "1. Jupyter notebook installed (obviously :)\n",
    "2. latest docker version installed\n",
    "    - for windows: https://docs.docker.com/desktop/windows/install/\n",
    "    - for macOS: https://docs.docker.com/desktop/mac/install/\n",
    "    - for linux: you can run the engine without the workaround\n",
    "3. git (how to: https://git-scm.com/book/en/v2/Getting-Started-Installing-Git)\n",
    "\n",
    "For Windows user: Anaconda comes with pip, jupyter and git (https://docs.anaconda.com/anaconda/install/windows/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "massive-baker",
   "metadata": {},
   "source": [
    "### use the following lines as an example to create a .env file in the root directory of RaiseWikibase\n",
    "\n",
    "change Passwords accordingly"
   ]
  },
  {
   "cell_type": "raw",
   "id": "worthy-eating",
   "metadata": {},
   "source": [
    "## Example / Template .env file for Wikibase release pipeline docker-compose example\n",
    "# WARNING: Do not add comments on the same line as env vars, as in some environments they will be included in the var!\n",
    "\n",
    "## Image Configuration\n",
    "WIKIBASE_IMAGE_NAME=wikibase/wikibase:1.35.4-wmde.2\n",
    "WDQS_IMAGE_NAME=wikibase/wdqs:0.3.40-wmde.2\n",
    "WDQS_FRONTEND_IMAGE_NAME=wikibase/wdqs-frontend:wmde.2\n",
    "ELASTICSEARCH_IMAGE_NAME=wikibase/elasticsearch:6.5.4-wmde.2\n",
    "WIKIBASE_BUNDLE_IMAGE_NAME=wikibase/wikibase-bundle:1.35.4-wmde.2\n",
    "QUICKSTATEMENTS_IMAGE_NAME=wikibase/quickstatements:wmde.2\n",
    "WDQS_PROXY_IMAGE_NAME=wikibase/wdqs-proxy:wmde.2\n",
    "MYSQL_IMAGE_NAME=mariadb:10.3\n",
    "\n",
    "## Mediawiki Configuration\n",
    "## Admin password\n",
    "## Passwords must be at least 10 characters.\n",
    "## Your password must be different from your username.\n",
    "## Your password must not appear within your username.\n",
    "## The password entered is in a list of very commonly used passwords. Please choose a more unique password\n",
    "MW_ADMIN_PASS=changethispassword\n",
    "MW_ADMIN_NAME=Admin\n",
    "MW_ADMIN_EMAIL=admin@example.com\n",
    "MW_SECRET_KEY=some-secret-key\n",
    "\n",
    "## Jobrunner Configuration\n",
    "MAX_JOBS=1\n",
    "\n",
    "## Database Configuration\n",
    "DB_NAME=my_wiki\n",
    "DB_USER=sqluser\n",
    "DB_PASS=change-this-sqlpassword\n",
    "\n",
    "## Wikibase Configuration\n",
    "WIKIBASE_PINGBACK=false\n",
    "# wikibase.svc is the internal docker hostname, change this value to the public hostname\n",
    "WIKIBASE_HOST=wikibase.svc\n",
    "WIKIBASE_PORT=80\n",
    "\n",
    "## WDQS-frontend Configuration\n",
    "WDQS_FRONTEND_HOST=wdqs-frontend.svc\n",
    "WDQS_FRONTEND_PORT=8834\n",
    "BRAND_TITLE=BERD\n",
    "\n",
    "## Quickstatements Configuration\n",
    "# quickstatements.svc is the internal docker hostname, change this value to the public hostname\n",
    "QUICKSTATEMENTS_HOST=quickstatements.svc\n",
    "QUICKSTATEMENTS_PORT=8840\n",
    "\n",
    "## ElasticSearch\n",
    "MW_ELASTIC_HOST=elasticsearch.svc\n",
    "MW_ELASTIC_PORT=9200\n",
    "\n",
    "# OAUTH\n",
    "OAUTH_CONSUMER_KEY=dfkjs\n",
    "OAUTH_CONSUMER_SECRET=deivmd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assisted-charter",
   "metadata": {},
   "source": [
    "### Docker set up  (see https://github.com/UB-Mannheim/RaiseWikibase)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noticed-nomination",
   "metadata": {},
   "source": [
    "> docker-compose -f docker-compose.yml -f docker-compose.extra.yml up -d --scale wikibase_jobrunner=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civic-projector",
   "metadata": {},
   "source": [
    "Running\n",
    "\n",
    "> docker ps\n",
    "\n",
    "should result in something like this:\n",
    "https://github.com/UB-Mannheim/RaiseWikibase#:~:text=If%20it%27s%20running%2C%20the%20output%20looks%20like%20this%3A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electrical-projection",
   "metadata": {},
   "source": [
    "### ... and clean up\n",
    "\n",
    "run on line after another in /RaiseWikibase:\n",
    "> docker-compose down\n",
    "\n",
    "> sudo rm -rf mediawiki-*  query-service-data/ quickstatements-data/\n",
    "\n",
    "> docker-compose up -d\n",
    "\n",
    "this removes the docker container, cleans the necessary storage places and mounts and starts the containers anew.\n",
    "\n",
    "Note: In the readme to the latest version of RaiseWikibase the second command is replaced by \"docker volume prune\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acquired-masters",
   "metadata": {},
   "source": [
    "## How it works\n",
    "\n",
    "the following cells demonstrate how to :\n",
    " 1. copying items/properties from wikidata\n",
    " 2. creating items from scratch in RaiseWikibase work\n",
    " \n",
    "Note: IDs of Qualifiers and Properties are erased, when downloaded from wikidata and get a new index. Look at your Wikibase to see, which ones are the right ones to use. You can find a list at: http://localhost/wiki/Special:AllPages?from=&to=&namespace=122&hideredirects=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unknown-appraisal",
   "metadata": {},
   "source": [
    "### set up a bot to fill your own wikibase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "korean-polymer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot is created. Metadata is saved to '.config.json'.\n"
     ]
    }
   ],
   "source": [
    "from RaiseWikibase.raiser import create_bot\n",
    "from RaiseWikibase.settings import Settings\n",
    "create_bot()\n",
    "config = Settings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dense-baptist",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_filename': '.config.json',\n",
       " 'username': 'MHuber@bot',\n",
       " 'password': 'tvqvf3mr20k14ati9j7obtcrq7mqhm31',\n",
       " 'mediawiki_api_url': 'http://localhost:8181/w/api.php',\n",
       " 'sparql_endpoint_url': 'http://localhost:8989/bigdata/sparql',\n",
       " 'wikibase_url': 'http://localhost:8181'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accepting-python",
   "metadata": {},
   "source": [
    "### get specific entity from wikidata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "respiratory-automation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from RaiseWikibase.raiser import batch, page\n",
    "from RaiseWikibase.datamodel import label, alias, description, snak, claim, entity, namespaces, datatypes\n",
    "import requests\n",
    "\n",
    "\n",
    "def get_wd_entity(wid=''):\n",
    "    \"\"\"Returns JSON representation of a Wikidata entity for the given WID\"\"\"\n",
    "    # Remove the following keys to avoid a problem with a new Wikibase instance\n",
    "    remove_keys = ['lastrevid', 'pageid', 'modified', 'title', 'ns']\n",
    "    #print(\"1\")\n",
    "    try:\n",
    "        r = requests.get('https://www.wikidata.org/entity/' + wid + '.json')\n",
    "        entity = r.json().get('entities').get(wid)\n",
    "        #print(entity)\n",
    "        for key in remove_keys:\n",
    "            entity.pop(key)\n",
    "            \n",
    "            #print(\"popped\")\n",
    "        #entity['claims'] = claim(prop='P1',\n",
    "         #                        mainsnak=snak(datatype='external-id',\n",
    "          #                                     value=wid,\n",
    "           #                                    prop='P1',\n",
    "            #                                   snaktype='value'),\n",
    "             #                    qualifiers=[],\n",
    "              #                   references=[])\n",
    "    except Exception:\n",
    "        print('stuff')\n",
    "        entity = None\n",
    "    return entity\n",
    "\n",
    "#add a list of wikidata qualifiers\n",
    "wids = ['Q386724']\n",
    "#wids = ['Q5','Q7725634','Q25372','Q1344','Q25379'] # human, work, play, drama, opera 'Q7725634','Q25379','Q25372','Q1344'\n",
    "items = [get_wd_entity(wid) for wid in wids]\n",
    "#print(items)\n",
    "\n",
    "#run batch to upload to your wikibase\n",
    "#batch('wikibase-item', items)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "universal-glasgow",
   "metadata": {},
   "source": [
    "### get specific property from wikidata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "needed-amber",
   "metadata": {},
   "outputs": [],
   "source": [
    "def property_wd(prop=''):\n",
    "    \"\"\"For the given PID of a property in Wikidata returns its \n",
    "    simplified JSON represetation (only the label, aliases, description \n",
    "    and one claim)\"\"\"\n",
    "    if prop:\n",
    "        r = requests.get('https://www.wikidata.org/entity/' + prop + '.json').json()\n",
    "        po = r.get('entities').get(prop)\n",
    "        p = entity(labels=restrict(po.get('labels')),\n",
    "                   aliases=restrict(po.get('aliases')),\n",
    "                   descriptions=restrict(po.get('descriptions')),\n",
    "                   claims=claim(prop='P1',\n",
    "                                mainsnak=snak(datatype='external-id',\n",
    "                                              value=prop,\n",
    "                                              prop='P1',\n",
    "                                              snaktype='value')),\n",
    "                   etype='property',\n",
    "                   datatype='string')\n",
    "    else:\n",
    "        p = None\n",
    "    return p\n",
    "#to upload: batch()..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dense-october",
   "metadata": {},
   "source": [
    "### Use RaiseWikibase \"from Scratch\"\n",
    " - vgl. https://www.wikidata.org/wiki/Wikidata:Glossary/en\n",
    " \n",
    "The following cells showcase how to construct an item/property with Raisewikibase\n",
    "Note: Properties (P...)and Qualifiers (Q...) are examples you need to adjust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "modular-stevens",
   "metadata": {},
   "outputs": [],
   "source": [
    "from RaiseWikibase.datamodel import label, alias, description, snak, claim, entity, statement\n",
    "from RaiseWikibase.raiser import batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foreign-boulder",
   "metadata": {},
   "source": [
    "### create label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "presidential-gothic",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = label('de','personitemlabel')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tough-border",
   "metadata": {},
   "source": [
    "### create alias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complete-laundry",
   "metadata": {},
   "outputs": [],
   "source": [
    "aliases = alias('de','alternativepersonlabel')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "located-dealing",
   "metadata": {},
   "source": [
    "### create descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fifth-thanksgiving",
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = description('de','Personenname gefunden in Theaterperiodika des 18. JH')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "located-yacht",
   "metadata": {},
   "source": [
    "### create a snak\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considered-massachusetts",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make them human (Q5 = human, P19 = instance of)\n",
    "mainsnak = snak(datatype='wikibase-item', value=\"Q5\", prop='P19', snaktype='value')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mounted-sessions",
   "metadata": {},
   "source": [
    "### create a claim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitting-hearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "claims =claim(prop='P19', mainsnak=mainsnak)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecological-combining",
   "metadata": {},
   "source": [
    "### create the entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legitimate-plasma",
   "metadata": {},
   "outputs": [],
   "source": [
    "item = entity(labels=labels,descriptions=descriptions,claims=claims,etype='item')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laden-zealand",
   "metadata": {},
   "source": [
    "### what an entity object, ready to upload looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "hazardous-salvation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'item',\n",
       " 'id': 'Q24500',\n",
       " 'labels': {'en': {'language': 'en', 'value': 'human'}},\n",
       " 'aliases': {'en': [{'language': 'en', 'value': 'person'}]},\n",
       " 'descriptions': {'en': {'language': 'en', 'value': 'a human being'}},\n",
       " 'claims': {'P1': [{'mainsnak': {'snaktype': 'value',\n",
       "     'property': 'P1',\n",
       "     'datavalue': {'value': 'Q5', 'type': 'string'},\n",
       "     'datatype': 'external-id',\n",
       "     'hash': 'bc7983b6-ce1b-4a4f-bc60-19691e7f6658'},\n",
       "    'type': 'statement',\n",
       "    'rank': 'normal',\n",
       "    'qualifiers': {'P1': []},\n",
       "    'qualifiers-order': ['P1'],\n",
       "    'references': [{'snaks': {'P1': []}, 'snaks-order': ['P1']}],\n",
       "    'id': 'Q24500$5fa9c6b6-1dcf-4c42-8bfc-dc66808dd4c0'}]}}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considerable-charm",
   "metadata": {},
   "source": [
    "### upload\n",
    "Note: to crate a new item/property use new=True,\n",
    "to change one, use new=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imperial-integer",
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch('wikibase-item', [item]], new=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broad-priority",
   "metadata": {},
   "source": [
    "### Basic example of how to add and change a property and/or a qualifier\n",
    "Code is from an example the developer wrote for an issue: https://github.com/UB-Mannheim/RaiseWikibase/issues/15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cardiac-adapter",
   "metadata": {},
   "outputs": [],
   "source": [
    "from RaiseWikibase.datamodel import label, alias, description, snak, claim, entity\n",
    "from RaiseWikibase.raiser import batch\n",
    "\n",
    "\n",
    "p = entity(labels=label(value='Wikidata ID'),\n",
    "           aliases=alias(value=[\"WID\", 'WikidataID']),\n",
    "           descriptions=description(value=\"ID of an entity in Wikidata\"),\n",
    "           claims={},\n",
    "           etype='property',\n",
    "           datatype='external-id')\n",
    "\n",
    "#batch('wikibase-property', [p])\n",
    "\n",
    "e = entity(labels=label(value='human'),\n",
    "           aliases={},\n",
    "           descriptions={},\n",
    "           claims={},\n",
    "           etype='item')\n",
    "\n",
    "#batch('wikibase-item', [e])\n",
    "\n",
    "m = entity(labels=label(value='human'),\n",
    "           aliases=alias(value=['person']),\n",
    "           descriptions=description(value='a human being'),\n",
    "           claims=claim(prop='P1',\n",
    "                        mainsnak=snak(datatype='external-id',\n",
    "                                      value='Q5',\n",
    "                                      prop='P1',\n",
    "                                      snaktype='value')),\n",
    "           etype='item')\n",
    "\n",
    "\n",
    "#important: an ID is mandatory\n",
    "#m[\"id\"]=\"Q24500\"\n",
    "#batch('wikibase-item', [m], new=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heavy-carolina",
   "metadata": {},
   "source": [
    "## Export and import Wikibase\n",
    "\n",
    "complete explanation found here: https://wikibase.consulting/transferring-wikibase-data-between-wikis/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "particular-royal",
   "metadata": {},
   "source": [
    "#### Export:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjustable-sleep",
   "metadata": {},
   "source": [
    "To create a xml dump, connect to wikibase docker container running:\n",
    "\n",
    "> docker-compose exec -it raisewikibase_wikibase_1 bash\n",
    "\n",
    "Note: raisewikibase_wikibase_1 is the name of the container\n",
    "\n",
    "with\n",
    "\n",
    "> php maintenance/dumpBackup.php --full --quiet > wikibase.xml\n",
    "\n",
    "you create a xml dump.\n",
    "\n",
    "To get the dump out of the container, navigate to a suitable location and run:\n",
    "\n",
    ">docker cp \"insertcontainernameorid\":/var/www/html/wikibase.xml .\n",
    "    \n",
    "Note:\n",
    "- you can also copy the container id (something like: 92bf73addb2c)<br>\n",
    "  you can find it as prefix to the bash window you opened to create the dump; e.g. root@92bf73addb2c:/var/www/html#\n",
    "- to list all container and ids use:\n",
    "    > docker ps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "universal-lobby",
   "metadata": {},
   "source": [
    "#### Import:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "russian-disorder",
   "metadata": {},
   "source": [
    "copy wikibase.xml to the root of the wikibase container.\n",
    "\n",
    "> docker cp wikibase.xml \"insertcontainernameorid\":/var/www/html/\n",
    "\n",
    "Enable Entity import by changing the localSettings file:\n",
    "\n",
    "- connect to container and start bash:\n",
    "\n",
    "> docker-compose exec -it raisewikibase_wikibase_1 bash\n",
    "\n",
    "- add \"$wgWBRepoSettings['allowEntityImport'] = true;\" to LocalSettings.php\n",
    "\n",
    "> echo \"$wgWBRepoSettings['allowEntityImport'] = true;\" >>> LocalSettings.php\n",
    "\n",
    "- to check if the line was appended run:\n",
    "\n",
    "> cat LocalSettings.php\n",
    "\n",
    "Start the import process by running following comands one after another:\n",
    "\n",
    "> php maintenance/importDump.php < ../wikibase.xml\n",
    "\n",
    "> php maintenance/rebuildall.php\n",
    "\n",
    "> php maintenance/runJobs.php\n",
    "\n",
    "> php maintenance/initSiteStats.php --update\n",
    "\n",
    "To finish up you need to run the following script inside your wikibase or you won't be able to manually create new items: https://gist.github.com/JeroenDeDauw/c86a5ab7e2771301eb506b246f1af7a6\n",
    "\n",
    "- Download the script or copy the lines below to a file named \"rebuildWikibaseIdCounters.sql\"\n",
    "\n",
    "- copy it into you wikibase container:\n",
    "\n",
    "> docker cp rebuildWikibaseIdCounters.sql \"insertcontainernameorid\":/var/www/html/maintenance\n",
    "\n",
    "- connect to the container and run the script:\n",
    "\n",
    "> docker-compose exec -it raisewikibase_wikibase_1 bash\n",
    "\n",
    "> php maintenance/sql.php rebuildWikibaseIdCounters.sql\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "healthy-applicant",
   "metadata": {},
   "source": [
    "-- By Jeroen De Dauw / https://Professional.Wiki\n",
    "-- License: GPL-2.0-or-later\n",
    "\n",
    "SELECT * FROM /*_*/wb_id_counters;\n",
    "\n",
    "REPLACE INTO /*_*/wb_id_counters VALUE((SELECT COALESCE(MAX(CAST(SUBSTRING(`page_title`, 2) AS UNSIGNED)), 0) FROM `page` WHERE `page_namespace` = 120), 'wikibase-item');\n",
    "\n",
    "REPLACE INTO /*_*/wb_id_counters VALUE((SELECT COALESCE(MAX(CAST(SUBSTRING(`page_title`, 2) AS UNSIGNED)), 0) FROM `page` WHERE `page_namespace` = 122), 'wikibase-property');\n",
    "\n",
    "SELECT * FROM /*_*/wb_id_counters;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funky-dressing",
   "metadata": {},
   "source": [
    "## Use dictionaries to populate your Wikibase\n",
    "\n",
    "The dictionaries have a name of a person or work as key and a list of references as values, e.g.:\n",
    "\n",
    "{<br>\n",
    "personname:<br>\n",
    "             [Sigle_Date_PAGE, Sigle_Date_PAGE, Sigle_Date_PAGE],<br>\n",
    "\n",
    "personname:<br>\n",
    "             [Sigle_Date_PAGE, Sigle_Date_PAGE, Sigle_Date_PAGE,]<br>\n",
    "\n",
    "             ...<br>\n",
    "}\n",
    "\n",
    "The following populates the local Wikibase instance step by step:\n",
    " 1. creates properties and qualifier as a layout for the datamodel used.\n",
    " 2. creates placeholder to later\n",
    "     - expand the data model\n",
    "     - add more data of already existing types (e.g. if you find more person or work names) within a pre defined range of QIDs\n",
    " 3. creates items for every name found in the dictionaries mentioned above\n",
    " 4. creates an dictionary for every *Sigle* found in the dictionaries mentioned above\n",
    " 4. links every them to the *Siglen*\n",
    " 5. links Theaterperiodika *Siglen* to all their manifestations (e.g. links ABB to ABB 1782, ABB 1785...)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artificial-carbon",
   "metadata": {},
   "source": [
    "## Some pre-processing steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alive-place",
   "metadata": {},
   "source": [
    "### create Dictionaries for \"Siglen\"\n",
    "\n",
    "Creates two dictionaries as preparation to ingest \"Siglen\" into the wikibase.\n",
    "\n",
    "1. {Sigle:description,...}\n",
    "2. {Sigle:[(SigleWithDate,),...]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compressed-declaration",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cheap-credit",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "import pandas as pd\n",
    "from RaiseWikibase.dbconnection import DBConnection\n",
    "\n",
    "def getLastID():\n",
    "    connection = DBConnection()\n",
    "    lastID = connection.get_last_eid(content_model='wikibase-item')\n",
    "    connection.conn.close()    \n",
    "    return lastID\n",
    "\n",
    "def getSiglenList(pathToList):\n",
    "    siglenList = pd.read_csv(pathToList, sep=\"\\t\", header=None)\n",
    "    siglenList = siglenList[0].tolist()\n",
    "    return siglenList\n",
    "\n",
    "def getSiglenDesc(pathToList):\n",
    "    siglenTable = pd.read_csv(pathToList, sep=\"\\t\", header=None)\n",
    "    siglenList = siglenTable[0].tolist()\n",
    "    siglenAufloesung = siglenTable[1].tolist()\n",
    "    return dict(zip(siglenList,siglenAufloesung))    \n",
    "\n",
    "def checkDictForSigle(sigle,dictwithSigle): \n",
    "    pattern = re.compile(sigle+\" \\d\\d\\d\\d\")\n",
    "    referencesOfSigle=[]\n",
    "    for key, value in dictwithSigle.items():\n",
    "        for entry in value:\n",
    "            match=re.match(pattern,entry)\n",
    "            if match and match.group(0) not in referencesOfSigle:\n",
    "                referencesOfSigle.append(match.group(0))\n",
    "    \n",
    "    return referencesOfSigle\n",
    "                \n",
    "            \n",
    "def createDict2(siglenlist, persondict, workdict,schlagwortdict):\n",
    "    dict2={}\n",
    "    personSigledict={}\n",
    "    workSigledict={}\n",
    "    schlagwortSigledict={}\n",
    "    for sigle in siglenlist:\n",
    "        \n",
    "        personSigleList = checkDictForSigle(sigle,persondict)\n",
    "        personSigledict[sigle] = personSigleList\n",
    "        workSigleList = checkDictForSigle(sigle,workdict)\n",
    "        workSigledict[sigle] = workSigleList\n",
    "        schlagwortSigleList = checkDictForSigle(sigle,schlagwortdict)\n",
    "        schlagwortSigledict[sigle] = schlagwortSigleList\n",
    "        combinedList = sorted(list(set((*personSigleList,*workSigleList,*schlagwortSigleList))))\n",
    "        #combinedandSortedList = combinedList.sort()\n",
    "        #dict2[sigle]=combinedandSortedList\n",
    "        dict2[sigle] = combinedList\n",
    "    return personSigledict,workSigledict,schlagwortSigledict,dict2\n",
    "\n",
    "\n",
    "def sortDict(dictToSort):\n",
    "    sorted_dict_temp = sorted(dictToSort.items(), key=lambda item: item[1])\n",
    "\n",
    "    sorted_dict = {k: v for k, v in sorted_dict_temp}\n",
    "    return sorted_dict\n",
    "\n",
    "def createPlaceHolderList(startID,endID):\n",
    "    from RaiseWikibase.datamodel import label, alias, description, snak, claim, entity\n",
    "    from RaiseWikibase.raiser import batch\n",
    "    #endID included\n",
    "    i = startID\n",
    "    placeholderList=[]\n",
    "    for qID in range(startID,endID+1):\n",
    "        e = entity(labels=label(value='This is placeholder Q'+str(qID)),\n",
    "           aliases={},\n",
    "           descriptions={},\n",
    "           claims={},\n",
    "           etype='item')\n",
    "        \n",
    "        placeholderList.append(e)\n",
    "    return(placeholderList)\n",
    "\n",
    "def createItems(listOfItemTitles):\n",
    "    itemsToAddList=[]\n",
    "    for itemTitle in listOfItemTitles:\n",
    "        itemtmp = entity(labels=label(value=itemTitle),\n",
    "           aliases={},\n",
    "           descriptions={},\n",
    "           claims={},\n",
    "           etype='item')\n",
    "        itemsToAddList.append(itemtmp)\n",
    "    return itemsToAddList\n",
    "\n",
    "\n",
    "def createProperties(listOfPropertyTitles):\n",
    "    propertyToAddList=[]\n",
    "    for propertyTitle in listOfPropertyTitles:\n",
    "        \n",
    "        propertytmp = entity(labels=label(value=propertyTitle),  \n",
    "                           aliases={},\n",
    "                           descriptions={},\n",
    "                           claims={},\n",
    "                           etype='property',\n",
    "                           datatype='wikibase-property')\n",
    "        propertyToAddList.append(propertytmp)\n",
    "    return propertyToAddList        \n",
    "\n",
    "def dumpjson(pathname, benderDict):\n",
    "    with open(pathname+\".json\",\"w\", encoding='utf-8') as jsonfile:\n",
    "        json.dump(benderDict,jsonfile, ensure_ascii=False)\n",
    "        print(\"dumped: \"+pathname+\".json\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strange-medication",
   "metadata": {},
   "source": [
    "### Create the Property \"subtype of\", and others...\n",
    "\n",
    "To change self defined properties to Wikidata-properties, use WikidataIntegrator + batch (vgl RaiseWikibase/Readme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cultural-ordering",
   "metadata": {},
   "outputs": [],
   "source": [
    "from RaiseWikibase.datamodel import label, alias, description, snak, claim, entity\n",
    "from RaiseWikibase.raiser import batch\n",
    "\n",
    "listOfPropertyTitles = ['instance of', 'subtype of', 'genre', 'author', 'described by source', 'stated in', 'field of work','sex or gender']\n",
    "\n",
    "propertyList= createProperties(listOfPropertyTitles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "every-ownership",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [00:00, 31.79it/s]\n"
     ]
    }
   ],
   "source": [
    "batch('wikibase-property', propertyList, new=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vulnerable-insulation",
   "metadata": {},
   "source": [
    "### Create describing Qualifiers (human, work etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "recreational-cooper",
   "metadata": {},
   "outputs": [],
   "source": [
    "listOfItemTitles=['human', 'work', 'Theaterperiodikum','female','schlagwort']\n",
    "\n",
    "\n",
    "itemList = createItems(listOfItemTitles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "prompt-volleyball",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00, 29.83it/s]\n"
     ]
    }
   ],
   "source": [
    "batch('wikibase-item',itemList)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sexual-malta",
   "metadata": {},
   "source": [
    "#### Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abstract-shock",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "95it [00:02, 36.03it/s]\n"
     ]
    }
   ],
   "source": [
    "placeholderList = createPlaceHolderList(6,100)\n",
    "\n",
    "batch('wikibase-item',placeholderList)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hidden-aquatic",
   "metadata": {},
   "source": [
    "### Create an item for each \"Theaterperiodikum\"\n",
    "also creates a dictionary to later map the subclasses of the Thaterperiodika to their \"parents\".\n",
    "\n",
    "dicttmp is a list with 3 dictionaries: \n",
    "1. a mapping of sigle: Sigle_With_Dates from the persons dictionary\n",
    "2. a mapping of sigle: Sigle_With_Dates from the works dictionary\n",
    "3. a mapping of sigle: Sigle_With_Dates of the combined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "satellite-character",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "siglenlist= getSiglenList(\"../data/Tabellen/Siglen_Aufloesung_Bender.tsv\")\n",
    "\n",
    "with open (\"../data/txt/Ergebnisse_OCR4all/finished/DramWerkregister_OCR4all.json\") as f:\n",
    "    workfile = json.load(f)\n",
    "    \n",
    "with open (\"../data/txt/Ergebnisse_OCR4all/finished/Gesamtregister_Personen_checked.json\") as f:\n",
    "    personfile = json.load(f)\n",
    "\n",
    "with open (\"../data/txt/Ergebnisse_OCR4all/finished/Gesamtregister_Schlagworte.json\") as f:\n",
    "    schlagwortefile = json.load(f)\n",
    "\n",
    "dicttmp = createDict2(siglenlist,personfile,workfile,schlagwortefile)\n",
    "\n",
    "#siglenDescDict is a mapping of Sigle:Sigle description\n",
    "siglenDescDict= getSiglenDesc(\"../data/Tabellen/Siglen_Aufloesung_Bender.tsv\")\n",
    "sorted_dict2 = sortDict(dicttmp[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "valid-coordinator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dumped: ../data/txt/zwischenergebnisse/referenceDict.json\n"
     ]
    }
   ],
   "source": [
    "sorted_dict2\n",
    "dumpjson('../data/txt/zwischenergebnisse/referenceDict', sorted_dict2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "composed-treasure",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adds Theaterperiodika (e.g \"ELB\", \"RUS\") as items\n",
    "siglenEntities=[]\n",
    "lastID = getLastID()\n",
    "i = lastID + 1\n",
    "sigleQID={} #stores Siglen:ID for referencing later\n",
    "\n",
    "for key in sorted_dict2:\n",
    "    e = entity(labels=label(value=key),\n",
    "               aliases={},\n",
    "               descriptions=description(value=siglenDescDict[key]),\n",
    "               claims=claim(prop='P1',\n",
    "                       mainsnak=snak(datatype='wikibase-item',\n",
    "                                     value='Q3', #item \"Theaterperiodikum\"\n",
    "                                     prop='P1',\n",
    "                                     snaktype='value')),\n",
    "               etype='item')\n",
    "    sigleQID[key]='Q'+str(i)\n",
    "    i+=1\n",
    "    siglenEntities.append(e)\n",
    "#siglenEntities   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excess-acoustic",
   "metadata": {},
   "source": [
    "### Dump the mapping sigle:QID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "thermal-ticket",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dumped: ../data/txt/zwischenergebnisse/siglen_QID_23122021.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "        \n",
    "dumpjson(\"../data/txt/zwischenergebnisse/siglen_QID_23122021\",sigleQID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "timely-february",
   "metadata": {},
   "source": [
    "### upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "mature-apple",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "153it [00:06, 24.27it/s]\n"
     ]
    }
   ],
   "source": [
    "batch('wikibase-item', siglenEntities, new=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retired-pollution",
   "metadata": {},
   "source": [
    "#### placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "contrary-coupon",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "247it [00:06, 36.76it/s]\n"
     ]
    }
   ],
   "source": [
    "startID = getLastID()+1\n",
    "placeholderList = createPlaceHolderList(startID,500)\n",
    "\n",
    "batch('wikibase-item',placeholderList)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spoken-school",
   "metadata": {},
   "source": [
    "### Creates entities for every published \"Theaterperiodikum\" and links them to their \"parents\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "federal-watershed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adds Theaterperiodika with dates(e.g \"ELB 1800\", \"RUS 1755\") as items\n",
    "siglenWithDatesEntities=[]\n",
    "lastID = getLastID()\n",
    "i = lastID + 1\n",
    "\n",
    "sigleWithDatesQID={} #stores Siglen:ID for referencing later\n",
    "for key, value in sorted_dict2.items():\n",
    "    for sigleWithDate in value:\n",
    "        if sigleWithDate:\n",
    "            e = entity(labels=label(value=sigleWithDate),\n",
    "                       aliases={},\n",
    "                       descriptions=description(value=siglenDescDict[key]),\n",
    "                       claims=claim(prop='P2',\n",
    "                               mainsnak=snak(datatype='wikibase-item',\n",
    "                                             value=sigleQID[key],\n",
    "                                             prop='P2',\n",
    "                                             snaktype='value')),\n",
    "                       etype='item')\n",
    "            sigleWithDatesQID[sigleWithDate]='Q'+str(i)\n",
    "            i+=1\n",
    "            siglenWithDatesEntities.append(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "composite-highlight",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dumped: ../data/txt/zwischenergebnisse/siglenWithDates_QID_23122021.json\n"
     ]
    }
   ],
   "source": [
    "def dumpjson(pathname, benderDict):\n",
    "    with open(pathname+\".json\",\"w\", encoding='utf-8') as jsonfile:\n",
    "        json.dump(benderDict,jsonfile, ensure_ascii=False)\n",
    "        print(\"dumped: \"+pathname+\".json\" )\n",
    "        \n",
    "dumpjson(\"../data/txt/zwischenergebnisse/siglenWithDates_QID_23122021\",sigleWithDatesQID)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supreme-gardening",
   "metadata": {},
   "source": [
    "### upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "asian-default",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "265it [00:08, 31.38it/s]\n"
     ]
    }
   ],
   "source": [
    "batch('wikibase-item', siglenWithDatesEntities, new=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cheap-constraint",
   "metadata": {},
   "source": [
    "#### placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "prime-scoop",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1235it [00:40, 30.68it/s]\n"
     ]
    }
   ],
   "source": [
    "startID = getLastID()+1\n",
    "placeholderList = createPlaceHolderList(startID,2000)\n",
    "\n",
    "batch('wikibase-item',placeholderList)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naughty-chase",
   "metadata": {},
   "source": [
    "## Prepare dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "governing-elder",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open (\"../data/txt/Ergebnisse_OCR4all/finished/DramWerkregister_OCR4all.json\") as f:\n",
    "    workfile = json.load(f)\n",
    "    \n",
    "with open (\"../data/txt/Ergebnisse_OCR4all/finished/Gesamtregister_Personen_checked.json\") as f:\n",
    "    personfile = json.load(f)\n",
    "\n",
    "with open (\"../data/txt/Ergebnisse_OCR4all/finished/Gesamtregister_Personen_isFemale.json.json\") as f:\n",
    "    personIsFemaleFile = json.load(f)\n",
    "with open (\"../data/txt/Ergebnisse_OCR4all/finished/Gesamtregister_Schlagworte.json\") as f:\n",
    "    schlagwortefile = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overhead-maine",
   "metadata": {},
   "source": [
    "### Persons\n",
    "adds all Persons from Bender's person name register and links them to all Theaterperiodika (sigle with Dates) they appear in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "declared-empire",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictToLinkPerson =[]\n",
    "sigleToSiglenListPersons = sortDict(dicttmp[0])\n",
    "femaleNameList = list(personIsFemaleFile.keys())\n",
    "for personname, referenceList in personfile.items():\n",
    "    claims={}\n",
    "    for reference in referenceList:\n",
    "        for sigle, sigleListWithDates in sigleToSiglenListPersons.items():\n",
    "            for sigleWithDate in sigleListWithDates:\n",
    "                if sigleWithDate in reference:\n",
    "                    if not claims:\n",
    "                        claims=claim(prop='P6',\n",
    "                               mainsnak=snak(datatype='wikibase-item',\n",
    "                                             value=sigleWithDatesQID[sigleWithDate],\n",
    "                                             prop='P6',\n",
    "                                             snaktype='value'))\n",
    "                    \n",
    "                    else:\n",
    "                        tmpclaim=claim(prop='P6',\n",
    "                               mainsnak=snak(datatype='wikibase-item',\n",
    "                                             value=sigleWithDatesQID[sigleWithDate],\n",
    "                                             prop='P6',\n",
    "                                             snaktype='value'))\n",
    "                                       \n",
    "                        claims['P6'].extend(tmpclaim['P6'])\n",
    "    if personname in femaleNameList:\n",
    "        claimIsFemale = claim(prop='P8',\n",
    "                                     mainsnak=snak(datatype='wikibase-item',\n",
    "                                                   value='Q4', #Q.. for female\n",
    "                                                   prop='P8', #P... sex or gender\n",
    "                                                   snaktype='value'))\n",
    "        claims = claims | claimIsFemale\n",
    "        \n",
    "                        \n",
    "    claimIsHuman = claim(prop='P1',\n",
    "                                 mainsnak=snak(datatype='wikibase-item',\n",
    "                                               value='Q1', #Q.. for human\n",
    "                                               prop='P1',\n",
    "                                               snaktype='value'))                \n",
    "                    \n",
    "    claims = claims | claimIsHuman\n",
    "    \n",
    "    e = entity(labels=label(value=personname),\n",
    "                aliases={},\n",
    "                descriptions={},\n",
    "                claims=claims,\n",
    "                etype='item')\n",
    "    #e[\"id\"]=\"Q\"+str(i)\n",
    "                    \n",
    "    dictToLinkPerson.append(e)\n",
    "    #i+=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "european-installation",
   "metadata": {},
   "source": [
    "### upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "coupled-music",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24495it [17:21, 23.51it/s]\n"
     ]
    }
   ],
   "source": [
    "batch('wikibase-item', dictToLinkPerson, new=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prostate-karaoke",
   "metadata": {},
   "source": [
    "#### placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "painful-hawaiian",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3505it [02:17, 25.41it/s]\n"
     ]
    }
   ],
   "source": [
    "startID = getLastID()+1\n",
    "placeholderList = createPlaceHolderList(startID,30000)\n",
    "\n",
    "batch('wikibase-item',placeholderList)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stylish-nerve",
   "metadata": {},
   "source": [
    "### Works\n",
    "adds all Works from Bender's dramatic work register and links them to all Theaterperiodika (sigle with Dates) they appear in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "labeled-madonna",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictToLinkWorks = []\n",
    "sigleToSiglenListWorks = sortDict(dicttmp[1])\n",
    "for workname, referenceList in workfile.items():\n",
    "    claims={}\n",
    "    for reference in referenceList:\n",
    "        for sigle, sigleListWithDates in sigleToSiglenListWorks.items():\n",
    "            for sigleWithDate in sigleListWithDates:\n",
    "                if sigleWithDate in reference:\n",
    "                    if not claims:\n",
    "                        claims=claim(prop='P6',\n",
    "                               mainsnak=snak(datatype='wikibase-item',\n",
    "                                             value=sigleWithDatesQID[sigleWithDate],\n",
    "                                             prop='P6',\n",
    "                                             snaktype='value'))\n",
    "                    \n",
    "                    else:\n",
    "                        tmpclaim=claim(prop='P6',\n",
    "                               mainsnak=snak(datatype='wikibase-item',\n",
    "                                             value=sigleWithDatesQID[sigleWithDate],\n",
    "                                             prop='P6',\n",
    "                                             snaktype='value'))\n",
    "                                       \n",
    "                        claims['P6'].extend(tmpclaim['P6'])\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "    #add claim workname is instance of work\n",
    "    claimIsWork = claim(prop='P1',\n",
    "                                 mainsnak=snak(datatype='wikibase-item',\n",
    "                                               value='Q2',#value = Q... for work\n",
    "                                               prop='P1',\n",
    "                                               snaktype='value'))\n",
    "    \n",
    "    \n",
    "    claims = claims | claimIsWork\n",
    "    #create entity\n",
    "    e = entity(labels=label(value=workname),\n",
    "                aliases={},\n",
    "                descriptions={},\n",
    "                claims=claims,\n",
    "                etype='item')\n",
    "    #e[\"id\"]=\"Q\"+str(i)\n",
    "                    \n",
    "    dictToLinkWorks.append(e)\n",
    "    #i+=1\n",
    "                    \n",
    "#56412 ABB 1776                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rural-sailing",
   "metadata": {},
   "source": [
    "### upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "progressive-virus",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31760it [27:46, 19.06it/s]\n"
     ]
    }
   ],
   "source": [
    "batch('wikibase-item', dictToLinkWorks, new=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "second-bridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "startID = getLastID()+1\n",
    "placeholderList = createPlaceHolderList(startID,70000)\n",
    "\n",
    "batch('wikibase-item',placeholderList)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "curious-investment",
   "metadata": {},
   "source": [
    "### Schlagworte\n",
    "adds all 'schlagworte' from Bender's Schlagwortregister and links them to all Theaterperiodika (sigle with Dates) they appear in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appropriate-midnight",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "driving-equation",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictToLinkSchlagworte = []\n",
    "sigleToSiglenListSchlagworte = sortDict(dicttmp[2])\n",
    "for schlagwort, referenceList in schlagwortefile.items():\n",
    "    claims={}\n",
    "    for reference in referenceList:\n",
    "        for sigle, sigleListWithDates in sigleToSiglenListSchlagworte.items():\n",
    "            for sigleWithDate in sigleListWithDates:\n",
    "                if sigleWithDate in reference:\n",
    "                    if not claims:\n",
    "                        claims=claim(prop='P6',\n",
    "                               mainsnak=snak(datatype='wikibase-item',\n",
    "                                             value=sigleWithDatesQID[sigleWithDate],\n",
    "                                             prop='P6',\n",
    "                                             snaktype='value'))\n",
    "                    \n",
    "                    else:\n",
    "                        tmpclaim=claim(prop='P6',\n",
    "                               mainsnak=snak(datatype='wikibase-item',\n",
    "                                             value=sigleWithDatesQID[sigleWithDate],\n",
    "                                             prop='P6',\n",
    "                                             snaktype='value'))\n",
    "                                       \n",
    "                        claims['P6'].extend(tmpclaim['P6'])\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "    #add claim schlagwort is instance of schlagwort\n",
    "    claimIsSchlagwort = claim(prop='P1',\n",
    "                                 mainsnak=snak(datatype='wikibase-item',\n",
    "                                               value='Q5',#value = Q... for work\n",
    "                                               prop='P1',\n",
    "                                               snaktype='value'))\n",
    "    \n",
    "    \n",
    "    claims = claims | claimIsSchlagwort\n",
    "    #create entity\n",
    "    e = entity(labels=label(value=schlagwort),\n",
    "                aliases={},\n",
    "                descriptions={},\n",
    "                claims=claims,\n",
    "                etype='item')\n",
    "    #e[\"id\"]=\"Q\"+str(i)\n",
    "                    \n",
    "    dictToLinkSchlagworte.append(e)\n",
    "    #i+=1\n",
    "                    \n",
    "#56412 ABB 1776                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "present-philosophy",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "829it [01:24,  9.76it/s]\n"
     ]
    }
   ],
   "source": [
    "batch('wikibase-item', dictToLinkSchlagworte, new=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuffed-prophet",
   "metadata": {},
   "source": [
    "## Examples of viable SPARQL-Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "useful-blind",
   "metadata": {},
   "source": [
    "#### sucht alle Ausgaben aller Theaterperiodika heraus\n",
    "diese Liste ist hilfreich um speziellere Suchanfragen zu formulieren, da auch die IDs der einzelnen Items mit angezeigt wird"
   ]
  },
  {
   "cell_type": "raw",
   "id": "superior-publicity",
   "metadata": {},
   "source": [
    "Select distinct ?periodikumWithDate ?periodikumWithDateLabel ?periodikum ?periodikumLabel\n",
    "Where{\n",
    "\n",
    "  ?periodikumWithDate wdt:P2  ?periodikum.\n",
    "  ?periodikum wdt:P1 wd:Q3\n",
    "\n",
    "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n",
    "}\n",
    "ORDER BY ASC(?periodikumWithDate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blessed-proof",
   "metadata": {},
   "source": [
    "#### fgt man ein #defaultView:Tree hinzu bekommt man eine aufklappbare Tabelle"
   ]
  },
  {
   "cell_type": "raw",
   "id": "atomic-grass",
   "metadata": {},
   "source": [
    "#defaultView:Tree\n",
    "Select distinct ?periodikum ?periodikumLabel ?periodikumWithDate ?periodikumWithDateLabel \n",
    "Where{\n",
    "\n",
    "  ?periodikumWithDate wdt:P2  ?periodikum.\n",
    "  ?periodikum wdt:P1 wd:Q3\n",
    "\n",
    "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n",
    "}\n",
    "ORDER BY ASC(?periodikumWithDate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personal-sierra",
   "metadata": {},
   "source": [
    "#### sucht alle Ausgaben des Theaterperiodikums ATB"
   ]
  },
  {
   "cell_type": "raw",
   "id": "driving-platform",
   "metadata": {},
   "source": [
    "Select distinct ?periodikumWithDate ?periodikumWithDateLabel\n",
    "Where{\n",
    "\n",
    "  ?periodikumWithDate wdt:P2  wd:Q107.\n",
    "\n",
    "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abroad-hypothetical",
   "metadata": {},
   "source": [
    "#### sucht alle Personen, die in einer Ausgabe von ATB (Q107) zu finden ist."
   ]
  },
  {
   "cell_type": "raw",
   "id": "comic-knowing",
   "metadata": {},
   "source": [
    "Select distinct ?periodikumWithDate ?periodikumWithDateLabel ?personLabel ?werkLabel\n",
    "Where{\n",
    "  ?person wdt:P6 ?periodikumWithDate.\n",
    "  ?person wdt:P1 wd:Q1.\n",
    "\n",
    "  ?periodikumWithDate wdt:P2  wd:Q107.\n",
    "\n",
    "          \n",
    "\n",
    "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reasonable-christopher",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "raw",
   "id": "soviet-round",
   "metadata": {},
   "source": [
    "Fr diese Instanz relevante Ranges:\n",
    "Properties: P1 -\n",
    "Beschreibende Qualifier: Q1 - 100\n",
    "Theaterperiodika (Siglen): Q101 - 500\n",
    "Theaterperiodika (Siglen mit Datum): Q501-2000\n",
    "Personennamen: Q2001 - 30000\n",
    "Werknamen: 30001 - 70000\n",
    "Schlagworte: 70001 -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assisted-secretary",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
